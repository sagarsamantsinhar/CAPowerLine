"""
Wildfire Risk Assessment Model using Random Forest and Gradient Boosting
FIXED VERSION: Addresses temporal data leakage, model consistency, and proper validation
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.calibration import CalibratedClassifierCV
import joblib
import json
from datetime import datetime
import os
import shap


class FireRiskModel:
    """
    Machine Learning model for wildfire risk assessment.
    Uses a SINGLE UNIFIED REGRESSOR approach with calibrated risk levels.
    Fixes:
    - No classifier/regressor disagreement (unified model)
    - Time-series aware splitting (temporal ordering preserved)
    - Proper train/val/test splits with held-out test set
    - Conservative evaluation metrics
    """
    
    def __init__(self):
        # Single unified model (regressor only)
        self.rf_regressor = None
        self.gb_regressor = None
        self.best_regressor = None
        
        # Preprocessing
        self.scaler = StandardScaler()
        
        # Risk thresholds (derived from continuous risk scores)
        self.risk_thresholds = {
            'Low': (0.0, 0.25),
            'Moderate': (0.25, 0.55),
            'High': (0.55, 0.80),
            'Critical': (0.80, 1.0)
        }
        
        self.risk_levels = ['Low', 'Moderate', 'High', 'Critical']
        
        # Feature names
        self.feature_names = [
            'veg_height_m',
            'clearance_m',
            'temperature_c',
            'humidity_pct',
            'wind_speed_ms',
            'days_since_rain',
            'latitude',
            'longitude',
            'season',
            'vegetation_growth_rate',
            'distance_to_powerline',
            'veg_to_clearance_ratio',
            'fire_danger_index',
            'moisture_deficit'
        ]
        
        self.model_dir = os.path.join(os.path.dirname(__file__), 'models')
        os.makedirs(self.model_dir, exist_ok=True)
        
        # SHAP explainer (initialized after training)
        self.explainer = None
        self.background_data = None
    
    def _score_to_risk_level(self, score):
        """Convert continuous risk score to discrete risk level."""
        score = np.clip(score, 0, 1)
        if score < 0.25:
            return 'Low'
        elif score < 0.55:
            return 'Moderate'
        elif score < 0.80:
            return 'High'
        else:
            return 'Critical'
    
    def _score_to_probabilities(self, score):
        """
        Convert risk score to probability distribution over risk levels.
        Uses smoother Gaussian-like distributions for realistic confidence.
        """
        score = np.clip(score, 0, 1)
        
        # Smoother probability distribution using wider Gaussian curves
        # Reduced steepness from 4 to 2.5 for more realistic confidence
        probs = {
            'Low': np.exp(-((score - 0.125) ** 2) / 0.04),  # Peak at 0.125
            'Moderate': np.exp(-((score - 0.40) ** 2) / 0.04),  # Peak at 0.40
            'High': np.exp(-((score - 0.675) ** 2) / 0.04),  # Peak at 0.675
            'Critical': np.exp(-((score - 0.90) ** 2) / 0.04)  # Peak at 0.90
        }
        
        # Normalize to sum to 1
        total = sum(probs.values())
        if total > 0:
            probs = {k: v/total for k, v in probs.items()}
        else:
            probs = {'Low': 0.25, 'Moderate': 0.25, 'High': 0.25, 'Critical': 0.25}
        
        return probs
    
    def create_features(self, data):
        """
        Create feature vector from input data.
        
        Args:
            data: dict with keys:
                - veg_height_m: Vegetation height in meters
                - clearance_m: Clearance distance in meters
                - temperature_c: Temperature in Celsius (optional)
                - humidity_pct: Humidity percentage (optional)
                - wind_speed_ms: Wind speed in m/s (optional)
                - days_since_rain: Days since last rain (optional)
                - latitude: Location latitude (optional)
                - longitude: Location longitude (optional)
                - date: Date string (optional)
                
        Returns:
            numpy array of features
        """
        # Required fields
        veg_height = data.get('veg_height_m', 0)
        clearance = data.get('clearance_m', 0)
        
        # Optional environmental fields (defaults to moderate conditions)
        temperature = data.get('temperature_c', 25.0)
        humidity = data.get('humidity_pct', 50.0)
        wind_speed = data.get('wind_speed_ms', 5.0)
        days_since_rain = data.get('days_since_rain', 7)
        
        # Optional location fields
        latitude = data.get('latitude', 37.0)
        longitude = data.get('longitude', -120.0)
        
        # Temporal features
        if 'date' in data:
            try:
                date_obj = datetime.fromisoformat(data['date'])
                month = date_obj.month
            except:
                month = datetime.now().month
        else:
            month = datetime.now().month
        
        # Season (0=Winter, 1=Spring, 2=Summer, 3=Fall)
        if month in [12, 1, 2]:
            season = 0
        elif month in [3, 4, 5]:
            season = 1
        elif month in [6, 7, 8]:
            season = 2
        else:
            season = 3
        
        # Derived features
        vegetation_growth_rate = data.get('growth_rate', veg_height / 30.0)
        distance_to_powerline = max(0, clearance)
        veg_to_clearance_ratio = veg_height / (clearance + 0.1)
        
        # Fire danger index
        fire_danger_index = self._calculate_fire_danger(
            temperature, humidity, wind_speed, days_since_rain
        )
        
        # Moisture deficit
        moisture_deficit = max(0, (30 - humidity) / 30.0)
        
        features = [
            veg_height,
            clearance,
            temperature,
            humidity,
            wind_speed,
            days_since_rain,
            latitude,
            longitude,
            season,
            vegetation_growth_rate,
            distance_to_powerline,
            veg_to_clearance_ratio,
            fire_danger_index,
            moisture_deficit
        ]
        
        return np.array(features).reshape(1, -1)
    
    def _calculate_fire_danger(self, temp, humidity, wind, days_dry):
        """Calculate simplified fire danger index."""
        temp_factor = np.clip((temp - 15) / 30.0, 0, 1)
        humidity_factor = 1.0 - (humidity / 100.0)
        wind_factor = np.clip(wind / 20.0, 0, 1)
        drought_factor = np.clip(days_dry / 30.0, 0, 1)
        
        fdi = (0.3 * temp_factor + 0.3 * humidity_factor + 
               0.2 * wind_factor + 0.2 * drought_factor)
        
        return fdi
    
    def generate_training_data(self, n_samples=2000):
        """
        Generate synthetic training data with TEMPORAL ORDERING.
        
        Args:
            n_samples: Number of samples to generate
            
        Returns:
            tuple: (features_df, risk_scores, timestamps)
        """
        np.random.seed(42)
        
        print(f"Generating {n_samples} training samples with temporal ordering...")
        
        data_list = []
        risk_scores = []
        timestamps = []
        
        # Generate samples with timestamps (simulate data collection over time)
        base_timestamp = datetime(2023, 1, 1)
        
        for i in range(n_samples):
            # Simulate temporal progression (data collected over 2 years)
            days_elapsed = i * (730 / n_samples)  # 2 years = 730 days
            timestamp = base_timestamp.timestamp() + (days_elapsed * 86400)
            timestamps.append(timestamp)
            
            # Generate scenario based on risk level distribution
            scenario_type = np.random.choice(
                ['low', 'moderate', 'high', 'critical'],
                p=[0.4, 0.3, 0.2, 0.1]
            )
            
            if scenario_type == 'low':
                data = self._generate_low_risk_scenario()
                risk_score = np.random.uniform(0.0, 0.25)
            elif scenario_type == 'moderate':
                data = self._generate_moderate_risk_scenario()
                risk_score = np.random.uniform(0.25, 0.55)
            elif scenario_type == 'high':
                data = self._generate_high_risk_scenario()
                risk_score = np.random.uniform(0.55, 0.80)
            else:  # critical
                data = self._generate_critical_risk_scenario()
                risk_score = np.random.uniform(0.80, 1.0)
            
            features = self.create_features(data)
            data_list.append(features[0])
            risk_scores.append(risk_score)
        
        features_df = pd.DataFrame(data_list, columns=self.feature_names)
        
        print(f"‚úÖ Generated {len(features_df)} samples with temporal ordering")
        
        # Count risk levels
        risk_labels = [self._score_to_risk_level(s) for s in risk_scores]
        print(f"Risk distribution: {pd.Series(risk_labels).value_counts().to_dict()}")
        
        return features_df, risk_scores, timestamps
    
    def _generate_low_risk_scenario(self):
        """Generate low risk scenario."""
        return {
            'veg_height_m': np.random.uniform(0.1, 1.5),
            'clearance_m': np.random.uniform(5.0, 10.0),
            'temperature_c': np.random.uniform(15, 25),
            'humidity_pct': np.random.uniform(50, 80),
            'wind_speed_ms': np.random.uniform(1, 8),
            'days_since_rain': np.random.randint(0, 7),
            'latitude': np.random.uniform(35, 39),
            'longitude': np.random.uniform(-122, -119),
            'growth_rate': np.random.uniform(0.01, 0.05)
        }
    
    def _generate_moderate_risk_scenario(self):
        """Generate moderate risk scenario."""
        return {
            'veg_height_m': np.random.uniform(1.5, 3.5),
            'clearance_m': np.random.uniform(2.5, 5.0),
            'temperature_c': np.random.uniform(22, 32),
            'humidity_pct': np.random.uniform(30, 55),
            'wind_speed_ms': np.random.uniform(6, 14),
            'days_since_rain': np.random.randint(5, 15),
            'latitude': np.random.uniform(35, 39),
            'longitude': np.random.uniform(-122, -119),
            'growth_rate': np.random.uniform(0.05, 0.10)
        }
    
    def _generate_high_risk_scenario(self):
        """Generate high risk scenario."""
        return {
            'veg_height_m': np.random.uniform(3.5, 6.0),
            'clearance_m': np.random.uniform(1.0, 2.5),
            'temperature_c': np.random.uniform(30, 38),
            'humidity_pct': np.random.uniform(15, 35),
            'wind_speed_ms': np.random.uniform(12, 20),
            'days_since_rain': np.random.randint(12, 30),
            'latitude': np.random.uniform(35, 39),
            'longitude': np.random.uniform(-122, -119),
            'growth_rate': np.random.uniform(0.08, 0.15)
        }
    
    def _generate_critical_risk_scenario(self):
        """Generate critical risk scenario."""
        return {
            'veg_height_m': np.random.uniform(5.5, 9.0),
            'clearance_m': np.random.uniform(0.0, 1.5),
            'temperature_c': np.random.uniform(35, 45),
            'humidity_pct': np.random.uniform(5, 20),
            'wind_speed_ms': np.random.uniform(18, 30),
            'days_since_rain': np.random.randint(25, 60),
            'latitude': np.random.uniform(35, 39),
            'longitude': np.random.uniform(-122, -119),
            'growth_rate': np.random.uniform(0.12, 0.25)
        }
    
    def train(self, features_df, risk_scores, timestamps):
        """
        Train models with PROPER TEMPORAL SPLITTING.
        
        Args:
            features_df: DataFrame with features
            risk_scores: List of risk scores (0-1)
            timestamps: List of timestamps (for temporal ordering)
            
        Returns:
            dict: Training metrics on held-out test set
        """
        print("\n" + "="*60)
        print("üöÄ TRAINING FIRE RISK ASSESSMENT MODEL (FIXED VERSION)")
        print("="*60)
        
        print(f"\nüìä Dataset: {len(features_df)} samples, {len(features_df.columns)} features")
        print("‚úÖ Using temporal splitting (no data leakage)")
        print("‚úÖ Unified regressor approach (no classifier/regressor disagreement)")
        
        # Sort by timestamp to ensure temporal ordering
        sort_idx = np.argsort(timestamps)
        X = features_df.iloc[sort_idx].values
        y = np.array(risk_scores)[sort_idx]
        
        # Temporal split: 60% train, 20% validation, 20% test (in chronological order)
        n = len(X)
        train_end = int(0.6 * n)
        val_end = int(0.8 * n)
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        
        X_val = X[train_end:val_end]
        y_val = y[train_end:val_end]
        
        X_test = X[val_end:]
        y_test = y[val_end:]
        
        print(f"\nüìÖ Temporal Split:")
        print(f"   Train: {len(X_train)} samples (earliest 60%)")
        print(f"   Validation: {len(X_val)} samples (middle 20%)")
        print(f"   Test: {len(X_test)} samples (latest 20%, held-out)")
        
        # Scale features (fit on train only!)
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Train Random Forest Regressor
        print("\nüå≤ Training Random Forest Regressor...")
        self.rf_regressor = RandomForestRegressor(
            n_estimators=100,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        self.rf_regressor.fit(X_train_scaled, y_train)
        
        rf_val_pred = self.rf_regressor.predict(X_val_scaled)
        rf_val_r2 = r2_score(y_val, rf_val_pred)
        rf_val_mae = mean_absolute_error(y_val, rf_val_pred)
        print(f"   Validation R¬≤: {rf_val_r2:.4f}")
        print(f"   Validation MAE: {rf_val_mae:.4f}")
        
        # Train Gradient Boosting Regressor
        print("\nüöÄ Training Gradient Boosting Regressor...")
        self.gb_regressor = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=4,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42
        )
        self.gb_regressor.fit(X_train_scaled, y_train)
        
        gb_val_pred = self.gb_regressor.predict(X_val_scaled)
        gb_val_r2 = r2_score(y_val, gb_val_pred)
        gb_val_mae = mean_absolute_error(y_val, gb_val_pred)
        print(f"   Validation R¬≤: {gb_val_r2:.4f}")
        print(f"   Validation MAE: {gb_val_mae:.4f}")
        
        # Select best model based on validation set
        if gb_val_r2 > rf_val_r2:
            self.best_regressor = self.gb_regressor
            best_name = "Gradient Boosting"
            best_val_r2 = gb_val_r2
            best_val_mae = gb_val_mae
        else:
            self.best_regressor = self.rf_regressor
            best_name = "Random Forest"
            best_val_r2 = rf_val_r2
            best_val_mae = rf_val_mae
        
        print(f"\nüèÜ Best Model: {best_name}")
        
        # Evaluate on HELD-OUT test set
        print(f"\nüìä Evaluating on Held-Out Test Set:")
        test_pred = self.best_regressor.predict(X_test_scaled)
        test_r2 = r2_score(y_test, test_pred)
        test_mse = mean_squared_error(y_test, test_pred)
        test_mae = mean_absolute_error(y_test, test_pred)
        
        print(f"   Test R¬≤: {test_r2:.4f}")
        print(f"   Test MSE: {test_mse:.4f}")
        print(f"   Test MAE: {test_mae:.4f}")
        
        # Calculate risk level accuracy on test set
        test_pred_labels = [self._score_to_risk_level(s) for s in test_pred]
        test_true_labels = [self._score_to_risk_level(s) for s in y_test]
        test_accuracy = sum([1 for p, t in zip(test_pred_labels, test_true_labels) if p == t]) / len(y_test)
        print(f"   Risk Level Accuracy: {test_accuracy:.4f}")
        
        # Initialize SHAP explainer
        print("\nüîç Initializing SHAP Explainer...")
        self._initialize_shap_explainer(X_train_scaled)
        print("‚úÖ SHAP explainer ready")
        
        print("\n" + "="*60)
        print("‚úÖ TRAINING COMPLETE")
        print("="*60)
        
        metrics = {
            'model': best_name,
            'validation': {
                'r2': best_val_r2,
                'mae': best_val_mae
            },
            'test': {
                'r2': test_r2,
                'mse': test_mse,
                'mae': test_mae,
                'risk_accuracy': test_accuracy
            }
        }
        
        return metrics
    
    def _initialize_shap_explainer(self, X_train):
        """
        Initialize SHAP explainer for model interpretability.
        Uses a subset of training data as background for faster computation.
        
        Args:
            X_train: Scaled training data
        """
        # Use a sample of training data as background (100 samples for speed)
        n_background = min(100, len(X_train))
        self.background_data = X_train[:n_background]
        
        # Create TreeExplainer for tree-based models (faster than KernelExplainer)
        self.explainer = shap.TreeExplainer(
            self.best_regressor,
            self.background_data,
            feature_names=self.feature_names
        )
    
    def get_shap_values(self, data):
        """
        Compute SHAP values for a prediction to explain why the model
        made a specific prediction.
        
        Args:
            data: dict with input features
            
        Returns:
            dict: SHAP explanation
        """
        if self.explainer is None:
            return None
        
        # Create and scale features
        features = self.create_features(data)
        X_scaled = self.scaler.transform(features)
        
        # Compute SHAP values
        shap_values = self.explainer.shap_values(X_scaled)
        
        # Get base value (average model output)
        base_value = self.explainer.expected_value
        
        # Create feature contribution dict
        contributions = {}
        for i, feature_name in enumerate(self.feature_names):
            contributions[feature_name] = {
                'value': float(features[0][i]),
                'shap_value': float(shap_values[0][i]),
                'contribution_pct': float(abs(shap_values[0][i]) / (abs(shap_values[0]).sum() + 1e-10) * 100)
            }
        
        # Sort by absolute SHAP value (importance)
        sorted_contributions = dict(
            sorted(contributions.items(), 
                   key=lambda x: abs(x[1]['shap_value']), 
                   reverse=True)
        )
        
        return {
            'base_value': float(base_value),
            'prediction': float(base_value + shap_values[0].sum()),
            'contributions': sorted_contributions,
            'top_contributors': list(sorted_contributions.keys())[:5]
        }
    
    def predict(self, data, explain=False):
        """
        Predict risk level and score for given data.
        Uses UNIFIED MODEL approach - no disagreement possible.
        
        Args:
            data: dict with input features
            explain: bool, if True include SHAP explanations
            
        Returns:
            dict: Prediction results (with optional SHAP explanations)
        """
        if self.best_regressor is None:
            raise ValueError("Model not trained. Call train() first or load_models().")
        
        # Create features
        features = self.create_features(data)
        
        # Scale features
        X_scaled = self.scaler.transform(features)
        
        # Get risk score from regressor
        risk_score = float(self.best_regressor.predict(X_scaled)[0])
        risk_score = np.clip(risk_score, 0, 1)
        
        # Derive risk level from score (consistent by design)
        risk_level = self._score_to_risk_level(risk_score)
        
        # Get smooth probability distribution
        probabilities = self._score_to_probabilities(risk_score)
        
        # Confidence is the probability of the predicted level
        confidence = probabilities[risk_level]
        
        result = {
            'risk_level': risk_level,
            'risk_score': risk_score,
            'risk_percentage': risk_score * 100,
            'confidence': confidence,
            'probabilities': probabilities
        }
        
        # Add SHAP explanation if requested
        if explain and self.explainer is not None:
            result['explanation'] = self.get_shap_values(data)
        
        return result
    
    def save_models(self, prefix='fire_risk_model'):
        """Save trained model with SHAP components."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        model_files = {
            'regressor': os.path.join(self.model_dir, f'{prefix}_regressor_{timestamp}.pkl'),
            'scaler': os.path.join(self.model_dir, f'{prefix}_scaler_{timestamp}.pkl'),
            'background_data': os.path.join(self.model_dir, f'{prefix}_background_{timestamp}.pkl'),
            'metadata': os.path.join(self.model_dir, f'{prefix}_metadata_{timestamp}.json')
        }
        
        joblib.dump(self.best_regressor, model_files['regressor'])
        joblib.dump(self.scaler, model_files['scaler'])
        
        # Save background data for SHAP
        if self.background_data is not None:
            joblib.dump(self.background_data, model_files['background_data'])
        
        metadata = {
            'timestamp': timestamp,
            'feature_names': self.feature_names,
            'risk_levels': self.risk_levels,
            'risk_thresholds': self.risk_thresholds,
            'has_shap': self.explainer is not None
        }
        
        with open(model_files['metadata'], 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\n‚úÖ Model saved successfully (timestamp: {timestamp})")
        print(f"   SHAP support: {'Yes' if self.explainer else 'No'}")
        return timestamp
    
    def load_models(self, prefix='fire_risk_model', timestamp=None):
        """Load trained model with SHAP components."""
        if timestamp is None:
            # Find most recent
            import glob
            files = glob.glob(os.path.join(self.model_dir, f'{prefix}_regressor_*.pkl'))
            if not files:
                raise FileNotFoundError(f"No models found with prefix '{prefix}'")
            latest_file = max(files)
            # Extract timestamp: format is YYYYMMDD_HHMMSS
            parts = latest_file.split('_')
            timestamp = f"{parts[-2]}_{parts[-1].replace('.pkl', '')}"
        
        model_files = {
            'regressor': os.path.join(self.model_dir, f'{prefix}_regressor_{timestamp}.pkl'),
            'scaler': os.path.join(self.model_dir, f'{prefix}_scaler_{timestamp}.pkl'),
            'background_data': os.path.join(self.model_dir, f'{prefix}_background_{timestamp}.pkl'),
            'metadata': os.path.join(self.model_dir, f'{prefix}_metadata_{timestamp}.json')
        }
        
        self.best_regressor = joblib.load(model_files['regressor'])
        self.scaler = joblib.load(model_files['scaler'])
        
        # Load background data and reinitialize SHAP explainer
        if os.path.exists(model_files['background_data']):
            self.background_data = joblib.load(model_files['background_data'])
            self._initialize_shap_explainer(self.background_data)
        
        with open(model_files['metadata'], 'r') as f:
            metadata = json.load(f)
            self.feature_names = metadata['feature_names']
            self.risk_levels = metadata['risk_levels']
            self.risk_thresholds = metadata.get('risk_thresholds', self.risk_thresholds)
        
        print(f"‚úÖ Model loaded successfully (timestamp: {timestamp})")
        if self.explainer:
            print(f"‚úÖ SHAP explainer ready for interpretability")
        return True
